### Instructions
I have used no Regularizers and this is the base performance. The code can be run directly regardless of working diirectory as long as the code is not moved from its original location after unzipping. I have included Dataset and Pre-Trained Models. Simply comment out line 503 in order to avoid training again.

```python
# Experiment1(Data)
```

There are no plots associated with the experiment as there is no Hyperparameter tuning.<br>

### Results

I trained 2-Hidden Layer Model for 10 Epochs and 4-Hidden Layer Model for 20 Epochs. Their accuracy on Test Dataset after training:<br>

--------------------------
|Model   | Test Accuracy     |
|:--------:|:---------------:|
|2 Hidden Layers        | 84.10 %|
| 4 Hidden Layers       | 81.58 %|
--------------------------

Using K-NN on embeddings generated by the models, I get a similar accuracy, which demonstrates that the embeddings $v_1$ and $v_2$ learnt by models are meaningful and can be clustered. Accuracy of K-NN on Test Dataset obtained(with $K = 15$):<br> 

--------------------------
|Embeddings   | Test Accuracy     |
|:--------:|:---------------:|
|2 Hidden Layers        | 84.84 %|
| 4 Hidden Layers       | 82.59 %|
--------------------------

### Remarks
As we can see, K-NN is able to produce higher test accuracies than the original models. Thus, we can conclude that the models have indeed learnt meaningfull representation of the data. But the reason accuracy is slightly higher than that of models is because we are removing thhe last linear layer. So the purpose of last linear layer is to **Linearly Separate** the embeddings into multiple classes. However, **K-NN is not restricted to Linearity**. This it can better separate the embeddings.